{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Replication and Extension] Titanic Project Example Walk Through \nIn this notebook, I hope to show how a data scientist would go about working through a problem. The goal is to correctly predict if someone survived the Titanic shipwreck. I thought it would be fun to see how well I could do in this competition w/ or w/o deep learning. \n\n## Overview \n### 1) Understand the shape of the data (Histograms, box plots, etc.)\n\n### 2) Data Cleaning \n\n### 3) Data Exploration\n\n### 4) Feature Engineering and Selection\n\n### 5) Data Preprocessing for Model\n\n### 6) Basic Model Building \n\n### 7) Model Tuning \n\n### 8) Ensemble Modle Building \n\n### 9) Deep Learning and its Hyper-param tuning\n\n### 10) Results ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T17:48:23.168823Z","iopub.execute_input":"2022-04-13T17:48:23.169176Z","iopub.status.idle":"2022-04-13T17:48:23.839625Z","shell.execute_reply.started":"2022-04-13T17:48:23.169146Z","shell.execute_reply":"2022-04-13T17:48:23.838307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we import the data. For this analysis, we will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, we will make predictions based on the test set. ","metadata":{}},{"cell_type":"code","source":"trainset = pd.read_csv('/kaggle/input/titanic/train.csv')\ntestset = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n# add bool variable to differentiate two datasets\ntrainset['train_test'] = 1\ntestset['train_test'] = 0\ntestset['Survived'] = np.NaN\nall_data = pd.concat([trainset,testset], axis=0)\n\n%matplotlib inline\nall_data.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:23.842658Z","iopub.execute_input":"2022-04-13T17:48:23.843174Z","iopub.status.idle":"2022-04-13T17:48:24.02439Z","shell.execute_reply.started":"2022-04-13T17:48:23.843124Z","shell.execute_reply":"2022-04-13T17:48:24.02344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Project Planning\nWhen starting any project, I like to outline the steps that I plan to take. Below is the rough outline that I created for this project using commented cells. ","metadata":{}},{"cell_type":"code","source":"# Understand nature of the data .info() .describe()\n# Histograms and boxplots \n# Value counts \n# Missing data \n# Correlation between the metrics \n# Explore interesting themes \n    # Wealthy survive? \n    # By location \n    # Age scatterplot with ticket price \n    # Young and wealthy Variable? \n    # Total spent? \n# Feature engineering \n# Preprocess data together or use a transformer? \n    # use label for train and test   \n# Scaling?\n\n# Model Baseline \n# Model comparison with CV \n\n# OO-style Perceptron implementation\n# OO-style FFN implementation","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:24.025577Z","iopub.execute_input":"2022-04-13T17:48:24.02604Z","iopub.status.idle":"2022-04-13T17:48:24.029778Z","shell.execute_reply.started":"2022-04-13T17:48:24.026008Z","shell.execute_reply":"2022-04-13T17:48:24.0287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gentle Data Exploration\n### 1) For numeric data \n* Made histograms to understand distributions \n* Corr-plot \n* Pivot table comparing survival rate across numeric variables \n\n\n### 2) For Categorical Data \n* Made bar charts to understand balance (or not) of classes \n* Made pivot tables to understand relationship with survival ","metadata":{}},{"cell_type":"code","source":"#quick look at our data types & null counts \ntrainset.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:24.030929Z","iopub.execute_input":"2022-04-13T17:48:24.031344Z","iopub.status.idle":"2022-04-13T17:48:24.055957Z","shell.execute_reply.started":"2022-04-13T17:48:24.031313Z","shell.execute_reply":"2022-04-13T17:48:24.05499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to better understand the numeric data, we want to use the .describe() method. This gives us an understanding of the central tendencies of the data \ntrainset.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:24.057493Z","iopub.execute_input":"2022-04-13T17:48:24.057986Z","iopub.status.idle":"2022-04-13T17:48:24.097615Z","shell.execute_reply.started":"2022-04-13T17:48:24.057948Z","shell.execute_reply":"2022-04-13T17:48:24.096889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#quick way to separate numeric columns using the desribe() func\ntrainset.describe().columns","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:45.002969Z","iopub.execute_input":"2022-04-13T17:48:45.0035Z","iopub.status.idle":"2022-04-13T17:48:45.030762Z","shell.execute_reply.started":"2022-04-13T17:48:45.003466Z","shell.execute_reply":"2022-04-13T17:48:45.029962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at numeric and categorical values separately \ndf_num = trainset[['Age','SibSp','Parch','Fare']]\ndf_cat = trainset[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:48:52.253035Z","iopub.execute_input":"2022-04-13T17:48:52.253684Z","iopub.status.idle":"2022-04-13T17:48:52.260745Z","shell.execute_reply.started":"2022-04-13T17:48:52.253647Z","shell.execute_reply":"2022-04-13T17:48:52.259987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#distributions for all numeric variables \nfor col_name in df_num.columns:\n    plt.hist(df_num[col_name])\n    plt.title(col_name)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:49:00.734681Z","iopub.execute_input":"2022-04-13T17:49:00.735351Z","iopub.status.idle":"2022-04-13T17:49:01.395525Z","shell.execute_reply.started":"2022-04-13T17:49:00.735311Z","shell.execute_reply":"2022-04-13T17:49:01.39436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perhaps we should take the non-normal distributions and consider normalizing them?","metadata":{"trusted":true}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"print(df_num.corr())\nsns.heatmap(df_num.corr())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:49:19.443615Z","iopub.execute_input":"2022-04-13T17:49:19.443986Z","iopub.status.idle":"2022-04-13T17:49:19.666563Z","shell.execute_reply.started":"2022-04-13T17:49:19.443954Z","shell.execute_reply":"2022-04-13T17:49:19.665524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare survival rate across Age, SibSp, Parch, and Fare; use surived col as the index, all other features as the columns\npd.pivot_table(trainset, index = 'Survived', values = df_num.columns)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:50:15.062735Z","iopub.execute_input":"2022-04-13T17:50:15.063128Z","iopub.status.idle":"2022-04-13T17:50:15.091567Z","shell.execute_reply.started":"2022-04-13T17:50:15.06309Z","shell.execute_reply":"2022-04-13T17:50:15.090891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Young ppl + high fare + low parch + high Sibsp = ? high likelihood to survive?","metadata":{}},{"cell_type":"code","source":"df_cat['Pclass'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:50:36.149218Z","iopub.execute_input":"2022-04-13T17:50:36.149796Z","iopub.status.idle":"2022-04-13T17:50:36.158573Z","shell.execute_reply.started":"2022-04-13T17:50:36.149759Z","shell.execute_reply":"2022-04-13T17:50:36.157717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col_name in df_cat.columns:\n    sns.barplot( df_cat[col_name].value_counts().index, df_cat[col_name].value_counts() ).set_title(col_name)\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:50:47.649118Z","iopub.execute_input":"2022-04-13T17:50:47.649691Z","iopub.status.idle":"2022-04-13T17:50:56.938495Z","shell.execute_reply.started":"2022-04-13T17:50:47.649639Z","shell.execute_reply":"2022-04-13T17:50:56.937259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin and ticket graphs are very messy. This is an area where we may want to do some feature engineering! ","metadata":{}},{"cell_type":"code","source":"# Comparing survival and each of these categorical variables \nprint(pd.pivot_table(trainset, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'), '\\n')\nprint(pd.pivot_table(trainset, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'), '\\n')\nprint(pd.pivot_table(trainset, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:51:10.223666Z","iopub.execute_input":"2022-04-13T17:51:10.224102Z","iopub.status.idle":"2022-04-13T17:51:10.280024Z","shell.execute_reply.started":"2022-04-13T17:51:10.224061Z","shell.execute_reply":"2022-04-13T17:51:10.278798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"higher class priority seems to lead a higher survived proportion.  \nlady first.  \nwhat does Embarked exactly means?!","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering and Selection\n### 1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n\n### 2) Tickets - Do different ticket types impact survival rates?\n\n### 3) Does a person's title relate to survival rates? ","metadata":{}},{"cell_type":"code","source":"df_cat.Cabin","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:51:49.333547Z","iopub.execute_input":"2022-04-13T17:51:49.333947Z","iopub.status.idle":"2022-04-13T17:51:49.342829Z","shell.execute_reply.started":"2022-04-13T17:51:49.333914Z","shell.execute_reply":"2022-04-13T17:51:49.341675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset['cabin_multiple'] = trainset.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number.","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:51:59.843717Z","iopub.execute_input":"2022-04-13T17:51:59.844098Z","iopub.status.idle":"2022-04-13T17:51:59.851954Z","shell.execute_reply.started":"2022-04-13T17:51:59.844065Z","shell.execute_reply":"2022-04-13T17:51:59.850814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset['cabin_multiple'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:52:00.183215Z","iopub.execute_input":"2022-04-13T17:52:00.183587Z","iopub.status.idle":"2022-04-13T17:52:00.192489Z","shell.execute_reply.started":"2022-04-13T17:52:00.183552Z","shell.execute_reply":"2022-04-13T17:52:00.191536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.pivot_table(trainset, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:52:23.997495Z","iopub.execute_input":"2022-04-13T17:52:23.998289Z","iopub.status.idle":"2022-04-13T17:52:24.023385Z","shell.execute_reply.started":"2022-04-13T17:52:23.998249Z","shell.execute_reply":"2022-04-13T17:52:24.022522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\n\ntrainset['cabin_adv'] = trainset.Cabin.apply(lambda x: str(x)[0])\ntrainset.cabin_adv","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:52:30.331237Z","iopub.execute_input":"2022-04-13T17:52:30.331941Z","iopub.status.idle":"2022-04-13T17:52:30.342594Z","shell.execute_reply.started":"2022-04-13T17:52:30.331903Z","shell.execute_reply":"2022-04-13T17:52:30.341293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#comparing surivial rate by cabin\nprint(trainset.cabin_adv.value_counts())\npd.pivot_table(trainset, index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:52:45.204082Z","iopub.execute_input":"2022-04-13T17:52:45.204542Z","iopub.status.idle":"2022-04-13T17:52:45.245097Z","shell.execute_reply.started":"2022-04-13T17:52:45.204509Z","shell.execute_reply":"2022-04-13T17:52:45.244013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#understand ticket values better \n#numeric vs non numeric \ntrainset['numeric_ticket'] = trainset.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrainset['ticket_letters'] = trainset.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:53:16.499781Z","iopub.execute_input":"2022-04-13T17:53:16.500247Z","iopub.status.idle":"2022-04-13T17:53:16.512151Z","shell.execute_reply.started":"2022-04-13T17:53:16.500211Z","shell.execute_reply":"2022-04-13T17:53:16.510712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset['numeric_ticket'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:53:17.27712Z","iopub.execute_input":"2022-04-13T17:53:17.277469Z","iopub.status.idle":"2022-04-13T17:53:17.286861Z","shell.execute_reply.started":"2022-04-13T17:53:17.277439Z","shell.execute_reply":"2022-04-13T17:53:17.285699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets us view all rows in dataframe through scrolling. This is for convenience \npd.set_option(\"max_rows\", None)\ntrainset['ticket_letters'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:53:38.243977Z","iopub.execute_input":"2022-04-13T17:53:38.244354Z","iopub.status.idle":"2022-04-13T17:53:38.255008Z","shell.execute_reply.started":"2022-04-13T17:53:38.244322Z","shell.execute_reply":"2022-04-13T17:53:38.253828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(trainset, index='Survived', columns='numeric_ticket', values = 'Ticket', aggfunc='count')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:53:47.888997Z","iopub.execute_input":"2022-04-13T17:53:47.889347Z","iopub.status.idle":"2022-04-13T17:53:47.915Z","shell.execute_reply.started":"2022-04-13T17:53:47.889317Z","shell.execute_reply":"2022-04-13T17:53:47.913684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#survival rate across different ticket types \npd.pivot_table(trainset, index='Survived', columns='ticket_letters', values = 'Ticket', aggfunc='count')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:53:56.476566Z","iopub.execute_input":"2022-04-13T17:53:56.47726Z","iopub.status.idle":"2022-04-13T17:53:56.514293Z","shell.execute_reply.started":"2022-04-13T17:53:56.477207Z","shell.execute_reply":"2022-04-13T17:53:56.512945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature engineering on person's title \ntrainset.Name.head(50)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:54:11.166771Z","iopub.execute_input":"2022-04-13T17:54:11.167139Z","iopub.status.idle":"2022-04-13T17:54:11.175893Z","shell.execute_reply.started":"2022-04-13T17:54:11.167108Z","shell.execute_reply":"2022-04-13T17:54:11.174771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset['name_title'] = trainset.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:54:17.026894Z","iopub.execute_input":"2022-04-13T17:54:17.027527Z","iopub.status.idle":"2022-04-13T17:54:17.035174Z","shell.execute_reply.started":"2022-04-13T17:54:17.027488Z","shell.execute_reply":"2022-04-13T17:54:17.033896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset['name_title'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:54:17.247535Z","iopub.execute_input":"2022-04-13T17:54:17.248326Z","iopub.status.idle":"2022-04-13T17:54:17.258755Z","shell.execute_reply.started":"2022-04-13T17:54:17.248281Z","shell.execute_reply":"2022-04-13T17:54:17.257022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing for Model \n### 1) Drop null values from Embarked (only 2) \n\n### 2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with) \nVariables:  'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n### 3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our training and test data have the same columns. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder). \n\n### 4) Impute data with mean for fare and age (Should also experiment with median) \n\n### 5) Normalized fare using logarithm to give more semblance of a normal distribution \n\n### 6) Scaled data 0-1 with standard scaler \n","metadata":{}},{"cell_type":"code","source":"#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data, using the info from train set\n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(trainset.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(trainset.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna( subset = ['Embarked'] , inplace = True )\n\n#tried log norm of sibsp (not used)\n# make input larger than 1, so the log is larger than 0\nall_data['norm_sibsp'] = np.log(all_data.SibSp + 1)\nplt.figure()\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log( all_data.Fare + 1)\nplt.figure()\nall_data['norm_fare'].hist()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:55:24.338721Z","iopub.execute_input":"2022-04-13T17:55:24.339172Z","iopub.status.idle":"2022-04-13T17:55:24.686988Z","shell.execute_reply.started":"2022-04-13T17:55:24.339132Z","shell.execute_reply":"2022-04-13T17:55:24.685728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converted to category type for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\nall_data.numeric_ticket = all_data.numeric_ticket.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','Parch','Embarked','train_test','cabin_adv','numeric_ticket','name_title','norm_sibsp','norm_fare']], drop_first=True)\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:56:08.151185Z","iopub.execute_input":"2022-04-13T17:56:08.151805Z","iopub.status.idle":"2022-04-13T17:56:08.184335Z","shell.execute_reply.started":"2022-04-13T17:56:08.151753Z","shell.execute_reply":"2022-04-13T17:56:08.183158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:56:09.204819Z","iopub.execute_input":"2022-04-13T17:56:09.205246Z","iopub.status.idle":"2022-04-13T17:56:09.212301Z","shell.execute_reply.started":"2022-04-13T17:56:09.205212Z","shell.execute_reply":"2022-04-13T17:56:09.211225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','norm_sibsp','Parch','norm_fare']] = scale.fit_transform(all_dummies_scaled[['Age','norm_sibsp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:58:27.809343Z","iopub.execute_input":"2022-04-13T17:58:27.809775Z","iopub.status.idle":"2022-04-13T17:58:27.895549Z","shell.execute_reply.started":"2022-04-13T17:58:27.809736Z","shell.execute_reply":"2022-04-13T17:58:27.894456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building (Baseline Validation Performance)\nBefore going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set. \n\n- Naive Bayes (72%)\n- Logistic Regression (82%)\n- Decision Tree (78%)\n- K Nearest Neighbor (82%)\n- Random Forest (81%)\n- Support Vector Classifier (83%)\n- Xtreme Gradient Boosting (82%)\n- Soft Voting Classifier - All Models (83%)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2022-04-13T17:58:37.167695Z","iopub.execute_input":"2022-04-13T17:58:37.16807Z","iopub.status.idle":"2022-04-13T17:58:37.713234Z","shell.execute_reply.started":"2022-04-13T17:58:37.16804Z","shell.execute_reply":"2022-04-13T17:58:37.712061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nRFE_estimator = LogisticRegression(max_iter = 1000)\nselector = RFE(RFE_estimator, n_features_to_select=15, step=1)\nselector = selector.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:02:22.688433Z","iopub.execute_input":"2022-04-13T18:02:22.688773Z","iopub.status.idle":"2022-04-13T18:02:23.553716Z","shell.execute_reply.started":"2022-04-13T18:02:22.688743Z","shell.execute_reply":"2022-04-13T18:02:23.552501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector.support_","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:02:36.305889Z","iopub.execute_input":"2022-04-13T18:02:36.306291Z","iopub.status.idle":"2022-04-13T18:02:36.313449Z","shell.execute_reply.started":"2022-04-13T18:02:36.306254Z","shell.execute_reply":"2022-04-13T18:02:36.312379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selector.ranking_","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:04:30.642097Z","iopub.execute_input":"2022-04-13T18:04:30.642644Z","iopub.status.idle":"2022-04-13T18:04:30.650451Z","shell.execute_reply.started":"2022-04-13T18:04:30.642609Z","shell.execute_reply":"2022-04-13T18:04:30.649447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train[list(np.array(X_train.columns)[selector.support_])]","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:09:59.034509Z","iopub.execute_input":"2022-04-13T18:09:59.034905Z","iopub.status.idle":"2022-04-13T18:09:59.042312Z","shell.execute_reply.started":"2022-04-13T18:09:59.034867Z","shell.execute_reply":"2022-04-13T18:09:59.041141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:21.404939Z","iopub.execute_input":"2022-04-13T18:10:21.405638Z","iopub.status.idle":"2022-04-13T18:10:21.453122Z","shell.execute_reply.started":"2022-04-13T18:10:21.405583Z","shell.execute_reply":"2022-04-13T18:10:21.451945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter = 1000)\ncv = cross_val_score(lr, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:37.935629Z","iopub.execute_input":"2022-04-13T18:10:37.936036Z","iopub.status.idle":"2022-04-13T18:10:38.071027Z","shell.execute_reply.started":"2022-04-13T18:10:37.936001Z","shell.execute_reply":"2022-04-13T18:10:38.069891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:39.644071Z","iopub.execute_input":"2022-04-13T18:10:39.644762Z","iopub.status.idle":"2022-04-13T18:10:39.701341Z","shell.execute_reply.started":"2022-04-13T18:10:39.64472Z","shell.execute_reply":"2022-04-13T18:10:39.700431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:41.891168Z","iopub.execute_input":"2022-04-13T18:10:41.891755Z","iopub.status.idle":"2022-04-13T18:10:41.995942Z","shell.execute_reply.started":"2022-04-13T18:10:41.891719Z","shell.execute_reply":"2022-04-13T18:10:41.99508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:43.352413Z","iopub.execute_input":"2022-04-13T18:10:43.352905Z","iopub.status.idle":"2022-04-13T18:10:44.387534Z","shell.execute_reply.started":"2022-04-13T18:10:43.35285Z","shell.execute_reply":"2022-04-13T18:10:44.386719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(probability = True)\ncv = cross_val_score(svc, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:46.122352Z","iopub.execute_input":"2022-04-13T18:10:46.122732Z","iopub.status.idle":"2022-04-13T18:10:46.851757Z","shell.execute_reply.started":"2022-04-13T18:10:46.122695Z","shell.execute_reply":"2022-04-13T18:10:46.850296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:49.190722Z","iopub.execute_input":"2022-04-13T18:10:49.191116Z","iopub.status.idle":"2022-04-13T18:10:50.605985Z","shell.execute_reply.started":"2022-04-13T18:10:49.191084Z","shell.execute_reply":"2022-04-13T18:10:50.604969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n# A \"soft\" classifier averages the confidence of each of the models. If the average confidence is > 50% that it is a 1， it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting='soft')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:52.917048Z","iopub.execute_input":"2022-04-13T18:10:52.917427Z","iopub.status.idle":"2022-04-13T18:10:52.927139Z","shell.execute_reply.started":"2022-04-13T18:10:52.917396Z","shell.execute_reply":"2022-04-13T18:10:52.926282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = cross_val_score(voting_clf, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:10:57.893781Z","iopub.execute_input":"2022-04-13T18:10:57.894565Z","iopub.status.idle":"2022-04-13T18:11:00.975473Z","shell.execute_reply.started":"2022-04-13T18:10:57.894511Z","shell.execute_reply":"2022-04-13T18:11:00.974349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_clf.fit(X_train_scaled, y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': testset.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:03.364233Z","iopub.execute_input":"2022-04-13T18:11:03.364572Z","iopub.status.idle":"2022-04-13T18:11:03.99369Z","shell.execute_reply.started":"2022-04-13T18:11:03.364544Z","shell.execute_reply":"2022-04-13T18:11:03.992718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Tuned Performance \nAfter getting the baselines, let's see if we can improve on the indivdual model results! I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time. \n\n|Model|Baseline|Tuned Performance|\n|-----|--------|-----------------|\n|Naive Bayes| 72%| NA|\n|Logistic Regression| 82%| 82%|\n|Decision Tree| 78%| NA|\n|K Nearest Neighbor| 82%| 82%|\n|Random Forest| 81%| 83% w/ RndSearch|\n|Support Vector Classifier| 83%| 83%|\n|Xtreme Gradient Boosting| 82%| 85% w/ RndSearch|","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV ","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:37.679031Z","iopub.execute_input":"2022-04-13T18:11:37.679386Z","iopub.status.idle":"2022-04-13T18:11:37.684561Z","shell.execute_reply.started":"2022-04-13T18:11:37.679356Z","shell.execute_reply":"2022-04-13T18:11:37.683256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    # input a fitted classifer with searchCV\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:39.829211Z","iopub.execute_input":"2022-04-13T18:11:39.829567Z","iopub.status.idle":"2022-04-13T18:11:39.834233Z","shell.execute_reply.started":"2022-04-13T18:11:39.829536Z","shell.execute_reply":"2022-04-13T18:11:39.833349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nclf_lr.fit(X_train_scaled, y_train)\nclf_performance(clf_lr, 'Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:40.619799Z","iopub.execute_input":"2022-04-13T18:11:40.620331Z","iopub.status.idle":"2022-04-13T18:11:44.912292Z","shell.execute_reply.started":"2022-04-13T18:11:40.620274Z","shell.execute_reply":"2022-04-13T18:11:44.911036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3, 5, 7, 9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nclf_knn.fit(X_train_scaled, y_train)\nclf_performance(clf_knn,'KNN')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:44.914969Z","iopub.execute_input":"2022-04-13T18:11:44.915773Z","iopub.status.idle":"2022-04-13T18:11:46.78013Z","shell.execute_reply.started":"2022-04-13T18:11:44.915716Z","shell.execute_reply":"2022-04-13T18:11:46.77896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1,0.5,1,2,5],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nclf_svc.fit(X_train_scaled, y_train)\nclf_performance(clf_svc,'SVC')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:11:46.782211Z","iopub.execute_input":"2022-04-13T18:11:46.783414Z","iopub.status.idle":"2022-04-13T18:14:28.24847Z","shell.execute_reply.started":"2022-04-13T18:11:46.783341Z","shell.execute_reply":"2022-04-13T18:14:28.247303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search around the best param region\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 30, cv = 5, verbose = True, n_jobs = -1)\nclf_rf_rnd.fit(X_train_scaled, y_train)\nclf_performance(clf_rf_rnd, 'Random Forest')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:31:41.9728Z","iopub.execute_input":"2022-04-11T01:31:41.973136Z","iopub.status.idle":"2022-04-11T01:32:33.437316Z","shell.execute_reply.started":"2022-04-11T01:31:41.973106Z","shell.execute_reply":"2022-04-11T01:32:33.436327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid =  {'n_estimators': [1000],\n               'criterion':['entropy'],\n                                  'bootstrap': [False],\n                                  'max_depth': [None],\n                                  'max_features': ['sqrt'],\n                                  'min_samples_leaf': [2,4,6],\n                                  'min_samples_split': [8,10,12]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nclf_rf.fit(X_train_scaled, y_train)\nclf_performance(clf_rf, 'Random Forest')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:36:23.419437Z","iopub.execute_input":"2022-04-11T01:36:23.419784Z","iopub.status.idle":"2022-04-11T01:36:55.701065Z","shell.execute_reply.started":"2022-04-11T01:36:23.419754Z","shell.execute_reply":"2022-04-11T01:36:55.700016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rf = clf_rf.best_estimator_.fit(X_train_scaled, y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:38:05.011824Z","iopub.execute_input":"2022-04-11T01:38:05.01215Z","iopub.status.idle":"2022-04-11T01:38:06.764442Z","shell.execute_reply.started":"2022-04-11T01:38:05.012125Z","shell.execute_reply":"2022-04-11T01:38:06.762587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 30, cv = 5, verbose = True, n_jobs = -1)\nclf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(clf_xgb_rnd,'XGB')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:39:47.373959Z","iopub.execute_input":"2022-04-11T01:39:47.374233Z","iopub.status.idle":"2022-04-11T01:40:04.961374Z","shell.execute_reply.started":"2022-04-11T01:39:47.374208Z","shell.execute_reply":"2022-04-11T01:40:04.960277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'n_estimators': [1000],\n    'colsample_bytree': [0.7,0.8,0.9],\n    'max_depth': [15,20,25],\n    'reg_alpha': [0.5],\n    'reg_lambda': [0.9,1,1.1],\n    'subsample': [0.7,0.9,1.1],\n    'learning_rate':[.01],\n    'gamma':[0.5,1,1.5],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\n                                  \nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nclf_xgb.fit(X_train_scaled, y_train)\nclf_performance(clf_xgb,'XGB')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:41:59.469746Z","iopub.execute_input":"2022-04-11T01:41:59.470131Z","iopub.status.idle":"2022-04-11T01:52:26.10603Z","shell.execute_reply.started":"2022-04-11T01:41:59.470098Z","shell.execute_reply":"2022-04-11T01:52:26.105319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_xgb = clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': testset.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:55:02.534853Z","iopub.execute_input":"2022-04-11T01:55:02.535205Z","iopub.status.idle":"2022-04-11T01:55:02.561017Z","shell.execute_reply.started":"2022-04-11T01:55:02.535176Z","shell.execute_reply":"2022-04-11T01:55:02.560261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Additional Ensemble Approaches \n1) Experimented with a hard voting classifier of three estimators (KNN, SVM, RF)\n\n2) Experimented with a soft voting classifier of three estimators (KNN, SVM, RF) (82.3%)\n\n3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC)\n\n4) Experimented with soft voting on all estimators including XGB (KNN, SVM, RF, LR, XGB)","metadata":{}},{"cell_type":"code","source":"best_lr = clf_lr.best_estimator_\nbest_knn = clf_knn.best_estimator_\nbest_svc = clf_svc.best_estimator_\nbest_rf = clf_rf.best_estimator_\nbest_xgb = clf_xgb.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:57:57.279027Z","iopub.execute_input":"2022-04-11T01:57:57.279416Z","iopub.status.idle":"2022-04-11T01:57:57.286065Z","shell.execute_reply.started":"2022-04-11T01:57:57.279385Z","shell.execute_reply":"2022-04-11T01:57:57.28421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:58:02.624017Z","iopub.execute_input":"2022-04-11T01:58:02.624414Z","iopub.status.idle":"2022-04-11T01:58:02.635665Z","shell.execute_reply.started":"2022-04-11T01:58:02.624383Z","shell.execute_reply":"2022-04-11T01:58:02.634393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\n#print('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\n#print('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\n#print('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T01:58:06.027364Z","iopub.execute_input":"2022-04-11T01:58:06.027878Z","iopub.status.idle":"2022-04-11T01:58:56.884324Z","shell.execute_reply.started":"2022-04-11T01:58:06.027843Z","shell.execute_reply":"2022-04-11T01:58:56.883522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n#no new results here\nparams = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nvote_weight.fit(X_train_scaled, y_train)\nclf_performance(vote_weight, 'VC Weights')\nvoting_clf_sub = vote_weight.best_estimator_.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:00:35.972164Z","iopub.execute_input":"2022-04-11T02:00:35.972605Z","iopub.status.idle":"2022-04-11T02:00:53.865313Z","shell.execute_reply.started":"2022-04-11T02:00:35.97257Z","shell.execute_reply":"2022-04-11T02:00:53.863511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make Predictions \nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\nbest_rf.fit(X_train_scaled, y_train)\n\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:03:25.891896Z","iopub.execute_input":"2022-04-11T02:03:25.892224Z","iopub.status.idle":"2022-04-11T02:03:37.465084Z","shell.execute_reply.started":"2022-04-11T02:03:25.892199Z","shell.execute_reply":"2022-04-11T02:03:37.463939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert output to dataframe \nfinal_data = {'PassengerId': testset.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': testset.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': testset.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': testset.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': testset.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': testset.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:03:37.466817Z","iopub.execute_input":"2022-04-11T02:03:37.467105Z","iopub.status.idle":"2022-04-11T02:03:37.480522Z","shell.execute_reply.started":"2022-04-11T02:03:37.467077Z","shell.execute_reply":"2022-04-11T02:03:37.479809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#track differences between outputs \n# find those data points with different predictions from various models\ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:03:37.481524Z","iopub.execute_input":"2022-04-11T02:03:37.481876Z","iopub.status.idle":"2022-04-11T02:03:37.562358Z","shell.execute_reply.started":"2022-04-11T02:03:37.481848Z","shell.execute_reply":"2022-04-11T02:03:37.561666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison.difference_rf_vc_hard.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:03:37.563281Z","iopub.execute_input":"2022-04-11T02:03:37.563646Z","iopub.status.idle":"2022-04-11T02:03:37.570646Z","shell.execute_reply.started":"2022-04-11T02:03:37.563618Z","shell.execute_reply":"2022-04-11T02:03:37.570037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare submission files \nsubmission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T02:04:27.657027Z","iopub.execute_input":"2022-04-11T02:04:27.657409Z","iopub.status.idle":"2022-04-11T02:04:27.67597Z","shell.execute_reply.started":"2022-04-11T02:04:27.657375Z","shell.execute_reply":"2022-04-11T02:04:27.674428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DL and its Hyper-param Tuning","metadata":{}},{"cell_type":"markdown","source":"# OO-style Perceptron implementation. This is a linear classifier!\n","metadata":{}},{"cell_type":"code","source":"class Eval:\n    def __init__(self, pred, gold):\n        self.pred = np.squeeze(pred)\n        self.gold = np.squeeze(gold)\n        \n    def Accuracy(self):\n        return np.sum(np.equal(self.pred, self.gold)) / float(len(self.gold))\n\nclass Perceptron:\n    def __init__(self, X, Y, N_ITERATIONS):\n        #TODO: Initalize parameters\n        self.lr = 1e-4\n        self.N_epochs = N_ITERATIONS\n        self.weights = np.zeros((X.shape[1],1)) # num_feats by 1\n        self.w_sum = np.zeros((X.shape[1],1)) # num_feats by 1\n        self.bias = 0\n        self.b_sum = 0\n        self.cnt = 1 # it is not simply the num_samples * num_epochs. since it is monotoniously increasing, the latest values are assigned with larger weights\n        self.Train(X,Y)\n\n    def ComputeAverageParameters(self):\n        #TODO: Compute average parameters (do this part last)\n        self.weights = self.weights - (self.w_sum / float(self.cnt))\n        self.bias = self.bias - (self.b_sum / float(self.cnt))\n        return\n\n    def Train(self, X, Y):\n        #TODO: Estimate perceptron parameters\n        for _ in range(self.N_epochs):\n            for inputs, label in zip(X, Y):\n                prediction = self.Predict(inputs.reshape(1,-1))\n                self.weights += self.lr * ((label - prediction) * inputs).reshape(-1,1)\n                self.w_sum += self.cnt * self.lr * ((label - prediction) * inputs).reshape(-1,1)\n                self.bias += self.lr * (label - prediction)\n                self.b_sum += self.cnt * self.lr * (label - prediction)\n            \n                self.cnt += 1\n\n        return\n\n    def Predict(self, X):\n        #TODO: Implement perceptron classification\n        out = np.dot(X, self.weights) + self.bias\n        return np.asarray([1 if out[i]>= 0.0 else -1 for i in range(X.shape[0])])\n\n    def SavePredictions(self, data, outFile):\n        Y_pred = self.Predict(data.X)\n        fOut = open(outFile, 'w')\n        for i in range(len(data.XfileList)):\n            fOut.write(f\"{data.XfileList[i]}\\t{Y_pred[i]}\\n\")\n\n    def Eval(self, X_test, Y_test):\n        Y_pred = self.Predict(X_test)\n        ev = Eval(Y_pred, Y_test)\n        return ev.Accuracy()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:40.417105Z","iopub.execute_input":"2022-04-13T18:14:40.417658Z","iopub.status.idle":"2022-04-13T18:14:40.437333Z","shell.execute_reply.started":"2022-04-13T18:14:40.417612Z","shell.execute_reply":"2022-04-13T18:14:40.435866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptron = Perceptron(X_train_scaled.values, y_train.values, 5) # train, just simply","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:41.08783Z","iopub.execute_input":"2022-04-13T18:14:41.088411Z","iopub.status.idle":"2022-04-13T18:14:41.250536Z","shell.execute_reply.started":"2022-04-13T18:14:41.088361Z","shell.execute_reply":"2022-04-13T18:14:41.249585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('in-sample accuracy', ptron.Eval(X_train_scaled.values, y_train.values))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:41.615461Z","iopub.execute_input":"2022-04-13T18:14:41.615811Z","iopub.status.idle":"2022-04-13T18:14:41.625448Z","shell.execute_reply.started":"2022-04-13T18:14:41.615781Z","shell.execute_reply":"2022-04-13T18:14:41.62439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ptron.ComputeAverageParameters() # calculated average parameters, then take off the mean values by shifting","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:42.392176Z","iopub.execute_input":"2022-04-13T18:14:42.392684Z","iopub.status.idle":"2022-04-13T18:14:42.396791Z","shell.execute_reply.started":"2022-04-13T18:14:42.392651Z","shell.execute_reply":"2022-04-13T18:14:42.395912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('in-sample accuracy', ptron.Eval(X_train_scaled.values, y_train.values)) # significant improvement!","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:44.10103Z","iopub.execute_input":"2022-04-13T18:14:44.101625Z","iopub.status.idle":"2022-04-13T18:14:44.11021Z","shell.execute_reply.started":"2022-04-13T18:14:44.101589Z","shell.execute_reply":"2022-04-13T18:14:44.109327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nclf_pcep = Perceptron(tol=1e-3, random_state=0)\ncv = cross_val_score(clf_pcep, X_train_scaled, y_train, cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:44.981754Z","iopub.execute_input":"2022-04-13T18:14:44.982394Z","iopub.status.idle":"2022-04-13T18:14:45.03381Z","shell.execute_reply.started":"2022-04-13T18:14:44.982357Z","shell.execute_reply":"2022-04-13T18:14:45.032683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_pcep.fit(X_train_scaled, y_train)\nclf_pcep.score(X_train_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:48.658279Z","iopub.execute_input":"2022-04-13T18:14:48.658628Z","iopub.status.idle":"2022-04-13T18:14:48.675013Z","shell.execute_reply.started":"2022-04-13T18:14:48.658599Z","shell.execute_reply":"2022-04-13T18:14:48.674056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OO-style FFN implementation. Keep as simple as possible and must do hyper-param searching!\n# use skorch to finish the 2-step from coarse to fine hyper-param search, then use your self-build pipleline to train and test the nn","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nif torch.cuda.is_available():\n    device = torch.device('cuda',0)\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:14:52.564177Z","iopub.execute_input":"2022-04-13T18:14:52.564532Z","iopub.status.idle":"2022-04-13T18:14:54.166811Z","shell.execute_reply.started":"2022-04-13T18:14:52.564496Z","shell.execute_reply":"2022-04-13T18:14:54.165741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the computation graph; one layer hidden network\nclass FFNN(nn.Module):\n    def __init__(self, dim_i, dim_h, dim_o):\n        super(FFNN, self).__init__()\n        self.V = nn.Linear(dim_i, dim_h)\n        self.g = nn.Tanh()\n        self.W = nn.Linear(dim_h, dim_o)\n        self.logSoftmax = nn.LogSoftmax(dim=0) # usually, the class dim is the last dim. here, we happened to find that there is only one dim, so use dim=0\n\n    def forward(self, x):\n        out = self.W(self.g(self.V(x)))\n        out = self.logSoftmax(out)\n        return out\n\ntrain_X = X_train_scaled.values\ntrain_Y = y_train.values\n\nnum_classes  = 2\nnum_hidden   = 10\nnum_features = train_X.shape[1]\n\nffnn = FFNN(num_features, num_hidden, num_classes).to(device)\noptimizer = optim.Adam(ffnn.parameters(), lr=1e-3)\n\nfor epoch in range(100):\n    total_loss = 0.0\n    #Randomly shuffle examples in each epoch\n    shuffled_i = list(range(0,len(train_Y)))\n    random.shuffle(shuffled_i)\n    for i in shuffled_i:\n        x        = torch.from_numpy(train_X[i]).float()\n        y_onehot = torch.zeros(num_classes)\n        y_onehot[int(train_Y[i])] = 1\n\n        logProbs = ffnn.forward(x)\n\n        #print(logProbs.shape, y_onehot.shape)\n        loss = torch.neg(logProbs).dot(y_onehot)\n        total_loss += loss\n        \n        ffnn.zero_grad()\n        loss.backward()\n        optimizer.step()\n    if epoch % 10 == 0:    \n        print(\"loss on epoch %i: %f\" % (epoch, total_loss))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:15:04.326031Z","iopub.execute_input":"2022-04-13T18:15:04.326738Z","iopub.status.idle":"2022-04-13T18:16:13.456058Z","shell.execute_reply.started":"2022-04-13T18:15:04.326689Z","shell.execute_reply":"2022-04-13T18:16:13.4551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate on the training set:\nnum_errors = 0\nfor i in range(len(train_Y)):\n    x = torch.from_numpy(train_X[i]).float()\n    y = train_Y[i]\n    logProbs = ffnn.forward(x)\n    prediction = torch.argmax(logProbs)\n    if y != prediction:\n        num_errors += 1\nprint(\"number of errors: %d\" % num_errors)\nprint(\"mis_ratio: %.2f\" % (num_errors/len(train_Y)))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:13.457888Z","iopub.execute_input":"2022-04-13T18:16:13.458198Z","iopub.status.idle":"2022-04-13T18:16:13.617419Z","shell.execute_reply.started":"2022-04-13T18:16:13.458167Z","shell.execute_reply":"2022-04-13T18:16:13.616235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Data Loaders\n\ntraining_set = torch.utils.data.TensorDataset(torch.Tensor(train_X), torch.Tensor(train_Y))\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:19.983546Z","iopub.execute_input":"2022-04-13T18:16:19.983929Z","iopub.status.idle":"2022-04-13T18:16:19.990879Z","shell.execute_reply.started":"2022-04-13T18:16:19.983896Z","shell.execute_reply":"2022-04-13T18:16:19.989959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:21.53727Z","iopub.execute_input":"2022-04-13T18:16:21.537968Z","iopub.status.idle":"2022-04-13T18:16:21.542769Z","shell.execute_reply.started":"2022-04-13T18:16:21.537927Z","shell.execute_reply":"2022-04-13T18:16:21.541376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, device, training_loader, optimizer, epoch):\n    # epoch means which epoch where are we, instead of total num of epochs\n    model.train() # declare train mode, so that we keep the grads\n    total_loss = 0\n    for idx, data in enumerate(training_loader, 0):\n        inputs, targets = data # a batch of data\n        \n        # add onehot encoder here! then the OO-style FFNN has been finished. just add some param-tune, then it is done.\n        ohe = OneHotEncoder()\n        targets = torch.tensor(ohe.fit_transform(targets.reshape(-1,1)).toarray(), dtype=torch.float32)\n        \n        # 1. Forward\n        outputs = model(inputs)\n        \n        #print(outputs, targets)\n        # 2. loss calculation, same dtype into the nn.MSELOSS\n        loss = criterion(outputs, targets)\n        \n        # 3. Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # 4. Comp grad\n        loss.backward()\n        \n        # 5. One step forward\n        optimizer.step()\n        \n        \n        total_loss += loss.item()\n    \n    print(\"Train Epoch: {}, Loss per batch: {}\".format(epoch, round(total_loss/len(training_loader), 4)))\n    train_loss_hist.append(total_loss/len(training_loader))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:44.012018Z","iopub.execute_input":"2022-04-13T18:16:44.012657Z","iopub.status.idle":"2022-04-13T18:16:44.021132Z","shell.execute_reply.started":"2022-04-13T18:16:44.012599Z","shell.execute_reply":"2022-04-13T18:16:44.019979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, device, testing_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for idx, data in enumerate(testing_loader, 0):\n            inputs, S_mat, C0 = data;\n    \n            # Forward\n            outputs = model(inputs)\n            \n            # Loss\n            loss = criterion(outputs * (S_mat[:,1:] - S_mat[:,0:1]) + C0, \n                             torch.max(S_mat[:,1:] - K, torch.zeros(batch_size, 1)))\n            \n            total_loss += loss.item() \n\n    print(\"Test loss per batch: {}\".format(round(total_loss/len(testing_loader),4)))\n    val_loss_hist.append(total_loss/len(testing_loader))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:44.497636Z","iopub.execute_input":"2022-04-13T18:16:44.498026Z","iopub.status.idle":"2022-04-13T18:16:44.505583Z","shell.execute_reply.started":"2022-04-13T18:16:44.497988Z","shell.execute_reply":"2022-04-13T18:16:44.504799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FFNN(nn.Module):\n    def __init__(self, dim_i, dim_h, dim_o):\n        super(FFNN, self).__init__()\n        self.V = nn.Linear(dim_i, dim_h)\n        self.g = nn.Tanh()\n        self.W = nn.Linear(dim_h, dim_o)\n        self.logSoftmax = nn.Softmax(dim=-1) # usually, the class dim is the last dim.\n\n    def forward(self, x):\n        out = self.W(self.g(self.V(x)))\n        out = self.logSoftmax(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:51.202958Z","iopub.execute_input":"2022-04-13T18:16:51.203767Z","iopub.status.idle":"2022-04-13T18:16:51.212668Z","shell.execute_reply.started":"2022-04-13T18:16:51.203711Z","shell.execute_reply":"2022-04-13T18:16:51.211875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FFNN(num_features, num_hidden, num_classes).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss()\ntrain_loss_hist = []\nfor epoch in range(10):\n    train(model, device, training_loader, optimizer, epoch)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:54.639587Z","iopub.execute_input":"2022-04-13T18:16:54.6403Z","iopub.status.idle":"2022-04-13T18:16:54.972278Z","shell.execute_reply.started":"2022-04-13T18:16:54.640249Z","shell.execute_reply":"2022-04-13T18:16:54.971116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nl1, = plt.plot(train_loss_hist)\n#l2, = plt.plot(val_loss_hist)\nplt.legend(handles=[l1], labels = ['train','val'], loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:16:59.103927Z","iopub.execute_input":"2022-04-13T18:16:59.104636Z","iopub.status.idle":"2022-04-13T18:16:59.247775Z","shell.execute_reply.started":"2022-04-13T18:16:59.104584Z","shell.execute_reply":"2022-04-13T18:16:59.246785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval() # do not update trainable params anymore\nnn_predictions = model(torch.Tensor(train_X))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:03.666076Z","iopub.execute_input":"2022-04-13T18:17:03.666583Z","iopub.status.idle":"2022-04-13T18:17:03.674485Z","shell.execute_reply.started":"2022-04-13T18:17:03.666551Z","shell.execute_reply":"2022-04-13T18:17:03.673371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_eval = Eval(torch.argmax(nn_predictions, dim=1).detach().numpy(), train_Y)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:07.974455Z","iopub.execute_input":"2022-04-13T18:17:07.974823Z","iopub.status.idle":"2022-04-13T18:17:07.980123Z","shell.execute_reply.started":"2022-04-13T18:17:07.974791Z","shell.execute_reply":"2022-04-13T18:17:07.978816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_eval.Accuracy()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:08.455417Z","iopub.execute_input":"2022-04-13T18:17:08.455959Z","iopub.status.idle":"2022-04-13T18:17:08.463379Z","shell.execute_reply.started":"2022-04-13T18:17:08.455916Z","shell.execute_reply":"2022-04-13T18:17:08.462293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Slightly change the loss func, pay attention to the inputs","metadata":{}},{"cell_type":"code","source":"class FFNN_ce(nn.Module):\n    def __init__(self, dim_i, dim_h, dim_o):\n        super(FFNN_ce, self).__init__()\n        self.V = nn.Linear(dim_i, dim_h)\n        self.g = nn.Tanh()\n        self.W = nn.Linear(dim_h, dim_o)\n        self.logSoftmax = nn.Softmax(dim=-1) # usually, the class dim is the last dim.\n\n    def forward(self, x):\n        out = self.W(self.g(self.V(x)))\n        #out = self.logSoftmax(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:19.152087Z","iopub.execute_input":"2022-04-13T18:17:19.152641Z","iopub.status.idle":"2022-04-13T18:17:19.16134Z","shell.execute_reply.started":"2022-04-13T18:17:19.152596Z","shell.execute_reply":"2022-04-13T18:17:19.160546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, device, training_loader, optimizer, epoch):\n    # epoch means which epoch where are we, instead of total num of epochs\n    model.train() # declare train mode, so that we keep the grads\n    total_loss = 0\n    for idx, data in enumerate(training_loader, 0):\n        inputs, targets = data # a batch of data\n        \n        # add onehot encoder here! then the OO-style FFNN has been finished. just add some param-tune, then it is done.\n        #ohe = OneHotEncoder()\n        #targets = torch.tensor(ohe.fit_transform(targets.reshape(-1,1)).toarray(), dtype=torch.float32)\n        \n        # 1. Forward\n        outputs = model(inputs) # just the logits\n        \n        # 2. input logits and original labels for ce\n        loss = criterion(outputs, targets.long())\n        \n        # 3. Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # 4. Comp grad\n        loss.backward()\n        \n        # 5. One step forward\n        optimizer.step()\n        \n        \n        total_loss += loss.item()\n    \n    print(\"Train Epoch: {}, Loss per batch: {}\".format(epoch, round(total_loss/len(training_loader), 4)))\n    train_loss_hist.append(total_loss/len(training_loader))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:19.982466Z","iopub.execute_input":"2022-04-13T18:17:19.983185Z","iopub.status.idle":"2022-04-13T18:17:19.992712Z","shell.execute_reply.started":"2022-04-13T18:17:19.983134Z","shell.execute_reply":"2022-04-13T18:17:19.991907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ce = FFNN_ce(num_features, num_hidden, num_classes).to(device)\noptimizer = optim.Adam(model_ce.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\ntrain_loss_hist = []\nfor epoch in range(10):\n    train(model_ce, device, training_loader, optimizer, epoch)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:23.025353Z","iopub.execute_input":"2022-04-13T18:17:23.026073Z","iopub.status.idle":"2022-04-13T18:17:23.212586Z","shell.execute_reply.started":"2022-04-13T18:17:23.026023Z","shell.execute_reply":"2022-04-13T18:17:23.211719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nl1, = plt.plot(train_loss_hist)\n#l2, = plt.plot(val_loss_hist)\nplt.legend(handles=[l1], labels = ['train','val'], loc='best')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:25.587655Z","iopub.execute_input":"2022-04-13T18:17:25.588032Z","iopub.status.idle":"2022-04-13T18:17:25.742211Z","shell.execute_reply.started":"2022-04-13T18:17:25.587988Z","shell.execute_reply":"2022-04-13T18:17:25.741123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ce.eval() # do not update trainable params anymore\nnn_predictions = model_ce(torch.Tensor(train_X))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:28.038686Z","iopub.execute_input":"2022-04-13T18:17:28.039372Z","iopub.status.idle":"2022-04-13T18:17:28.0453Z","shell.execute_reply.started":"2022-04-13T18:17:28.039332Z","shell.execute_reply":"2022-04-13T18:17:28.044223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_eval = Eval(torch.argmax(nn_predictions, dim=1).detach().numpy(), train_Y)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:40.83793Z","iopub.execute_input":"2022-04-13T18:17:40.838611Z","iopub.status.idle":"2022-04-13T18:17:40.843129Z","shell.execute_reply.started":"2022-04-13T18:17:40.838572Z","shell.execute_reply":"2022-04-13T18:17:40.842318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_eval.Accuracy()","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:17:50.902404Z","iopub.execute_input":"2022-04-13T18:17:50.902942Z","iopub.status.idle":"2022-04-13T18:17:50.909522Z","shell.execute_reply.started":"2022-04-13T18:17:50.902908Z","shell.execute_reply":"2022-04-13T18:17:50.90855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyper-param search","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:13:49.189051Z","iopub.execute_input":"2022-04-13T19:13:49.189459Z","iopub.status.idle":"2022-04-13T19:13:50.747639Z","shell.execute_reply.started":"2022-04-13T19:13:49.189426Z","shell.execute_reply":"2022-04-13T19:13:50.746448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect if we could use GPU\nif torch.cuda.is_available():\n    device = torch.device('cuda', 0)\nelse:\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:13:53.404217Z","iopub.execute_input":"2022-04-13T19:13:53.404632Z","iopub.status.idle":"2022-04-13T19:13:53.415747Z","shell.execute_reply.started":"2022-04-13T19:13:53.404593Z","shell.execute_reply":"2022-04-13T19:13:53.41441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassifierModule(nn.Module):\n    def __init__(\n            self,\n            num_units=10,\n            nonlin=F.relu,\n            dropout=0.5,\n    ):\n        super(ClassifierModule, self).__init__()\n        self.num_units = num_units\n        self.nonlin = nonlin\n        self.dropout = dropout\n\n        self.dense0 = nn.Linear(20, num_units)\n        self.nonlin = nonlin\n        self.dropout = nn.Dropout(dropout)\n        self.dense1 = nn.Linear(num_units, 10)\n        self.output = nn.Linear(10, 2)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.dropout(X)\n        X = F.relu(self.dense1(X))\n        X = F.softmax(self.output(X), dim=-1) # the last layers are a linear output and a softmax\n        return X","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:24:20.754081Z","iopub.execute_input":"2022-04-13T19:24:20.755047Z","iopub.status.idle":"2022-04-13T19:24:20.766084Z","shell.execute_reply.started":"2022-04-13T19:24:20.754996Z","shell.execute_reply":"2022-04-13T19:24:20.765075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U skorch","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:18:27.286906Z","iopub.execute_input":"2022-04-13T19:18:27.287349Z","iopub.status.idle":"2022-04-13T19:20:49.565866Z","shell.execute_reply.started":"2022-04-13T19:18:27.287306Z","shell.execute_reply":"2022-04-13T19:20:49.564593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import skorch\nfrom skorch import NeuralNetClassifier\nfrom skorch import NeuralNetRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:14:57.953734Z","iopub.execute_input":"2022-04-13T19:14:57.954065Z","iopub.status.idle":"2022-04-13T19:14:57.97917Z","shell.execute_reply.started":"2022-04-13T19:14:57.954037Z","shell.execute_reply":"2022-04-13T19:14:57.977863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = NeuralNetClassifier(\n    ClassifierModule,\n    max_epochs=20,\n    lr=0.1,\n    device=device,\n)\n# the self created ClassifierModule is a input param for the skorch wrapper","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:23.831687Z","iopub.execute_input":"2022-04-13T19:25:23.832063Z","iopub.status.idle":"2022-04-13T19:25:23.881073Z","shell.execute_reply.started":"2022-04-13T19:25:23.832031Z","shell.execute_reply":"2022-04-13T19:25:23.87958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:24.272542Z","iopub.execute_input":"2022-04-13T19:25:24.272906Z","iopub.status.idle":"2022-04-13T19:25:24.299717Z","shell.execute_reply.started":"2022-04-13T19:25:24.272869Z","shell.execute_reply":"2022-04-13T19:25:24.297976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = net.predict(X[:10])\ny_proba = net.predict_proba(X[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:28.718497Z","iopub.execute_input":"2022-04-13T19:25:28.719179Z","iopub.status.idle":"2022-04-13T19:25:28.745431Z","shell.execute_reply.started":"2022-04-13T19:25:28.719127Z","shell.execute_reply":"2022-04-13T19:25:28.744055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:28.966875Z","iopub.execute_input":"2022-04-13T19:25:28.967268Z","iopub.status.idle":"2022-04-13T19:25:29.792562Z","shell.execute_reply.started":"2022-04-13T19:25:28.967229Z","shell.execute_reply":"2022-04-13T19:25:29.791654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = NeuralNetClassifier(\n    ClassifierModule,\n    max_epochs = 20,\n    lr = 0.1,\n    optimizer__momentum = 0.9,\n    verbose = 0,\n    train_split = False,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:31.07406Z","iopub.execute_input":"2022-04-13T19:25:31.074641Z","iopub.status.idle":"2022-04-13T19:25:31.124973Z","shell.execute_reply.started":"2022-04-13T19:25:31.07459Z","shell.execute_reply":"2022-04-13T19:25:31.123684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'lr': [0.001, 0.01, 0.1],\n    'max_epochs': [10, 20, 30],\n    'module__num_units': [10, 20, 40],\n    'module__dropout': [0, 0.5],\n    'optimizer__nesterov': [False, True],\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:33.662239Z","iopub.execute_input":"2022-04-13T19:25:33.662981Z","iopub.status.idle":"2022-04-13T19:25:33.668898Z","shell.execute_reply.started":"2022-04-13T19:25:33.662939Z","shell.execute_reply":"2022-04-13T19:25:33.667849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs = RandomizedSearchCV(net, params, refit=False, cv=3, scoring='f1', verbose=2, n_iter = 20)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:34.311879Z","iopub.execute_input":"2022-04-13T19:25:34.312637Z","iopub.status.idle":"2022-04-13T19:25:34.337676Z","shell.execute_reply.started":"2022-04-13T19:25:34.312596Z","shell.execute_reply":"2022-04-13T19:25:34.336139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:34.790044Z","iopub.execute_input":"2022-04-13T19:25:34.79082Z","iopub.status.idle":"2022-04-13T19:25:34.819477Z","shell.execute_reply.started":"2022-04-13T19:25:34.790763Z","shell.execute_reply":"2022-04-13T19:25:34.817621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(rs.best_score_, rs.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:38.610653Z","iopub.execute_input":"2022-04-13T19:25:38.611432Z","iopub.status.idle":"2022-04-13T19:25:38.638797Z","shell.execute_reply.started":"2022-04-13T19:25:38.611379Z","shell.execute_reply":"2022-04-13T19:25:38.637421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    'lr': [0.1],\n    'max_epochs': [15, 20, 25],\n    'module__num_units': [30, 40, 50],\n    'module__dropout': [0],\n    'optimizer__nesterov': [True],\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:25:38.796736Z","iopub.execute_input":"2022-04-13T19:25:38.797124Z","iopub.status.idle":"2022-04-13T19:25:38.804365Z","shell.execute_reply.started":"2022-04-13T19:25:38.797071Z","shell.execute_reply":"2022-04-13T19:25:38.802451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs = GridSearchCV(net, params, refit=True, cv=3, scoring='f1', verbose=2) # with refit = True, to keep the best estimator","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:26:02.78549Z","iopub.execute_input":"2022-04-13T19:26:02.786207Z","iopub.status.idle":"2022-04-13T19:26:02.812512Z","shell.execute_reply.started":"2022-04-13T19:26:02.786164Z","shell.execute_reply":"2022-04-13T19:26:02.811272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:26:02.983055Z","iopub.execute_input":"2022-04-13T19:26:02.983648Z","iopub.status.idle":"2022-04-13T19:26:03.014292Z","shell.execute_reply.started":"2022-04-13T19:26:02.983598Z","shell.execute_reply":"2022-04-13T19:26:03.012522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(gs.best_score_, gs.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:26:03.456977Z","iopub.execute_input":"2022-04-13T19:26:03.457808Z","iopub.status.idle":"2022-04-13T19:26:03.488792Z","shell.execute_reply.started":"2022-04-13T19:26:03.457748Z","shell.execute_reply":"2022-04-13T19:26:03.486467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:26:04.061506Z","iopub.execute_input":"2022-04-13T19:26:04.061898Z","iopub.status.idle":"2022-04-13T19:26:04.090252Z","shell.execute_reply.started":"2022-04-13T19:26:04.061867Z","shell.execute_reply":"2022-04-13T19:26:04.087677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.predict(X[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T19:26:05.000825Z","iopub.execute_input":"2022-04-13T19:26:05.001583Z","iopub.status.idle":"2022-04-13T19:26:05.034354Z","shell.execute_reply.started":"2022-04-13T19:26:05.001525Z","shell.execute_reply":"2022-04-13T19:26:05.032581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs.predict_proba(X[:10])","metadata":{"execution":{"iopub.status.busy":"2022-04-13T18:27:33.356454Z","iopub.execute_input":"2022-04-13T18:27:33.356825Z","iopub.status.idle":"2022-04-13T18:27:33.38662Z","shell.execute_reply.started":"2022-04-13T18:27:33.356789Z","shell.execute_reply":"2022-04-13T18:27:33.385224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}