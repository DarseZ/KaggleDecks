{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本脚本进行under different scenario不同资产配置方法的蒙特卡洛随机模拟实验\n",
    "Approach1:看似每周调一次仓位，实则每个交易日调一次，调至最近一次周度计算出的权重值\n",
    "    Approach2:按照backtest中纯正周度实盘调仓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mpl\n",
    "import scipy.spatial.distance as ssd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_ratio(returns):\n",
    "    \"\"\"Determines the ratiro btw the right95% and left5% tail\n",
    "     Input:\n",
    "     returns: pd.Series\n",
    "          daily/weekly/monthly returns of strategy, noncumulative\n",
    "     Output:\n",
    "     tail ratio describing how good or bad the relative tails will lead to\n",
    "     \"\"\"\n",
    "    return np.abs(np.percentile(returns,95)) / np.abs(np.percentile(returns,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeVaR( ret_series, T = 10):\n",
    "    #计算VaR: alpha = 0.01\n",
    "    VaR_Tday = ret_series.mean() * T + (-2.33) * ret_series.std() * T**0.5\n",
    "    \n",
    "    return VaR_Tday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1\n",
    "def getClusterVar( cov,cItems ):\n",
    "    #compute variance per cluster\n",
    "    cov_ = cov.loc[ cItems,cItems ]  # matrix slice\n",
    "    w_ = getIVP(cov_).reshape(-1,1)  #设定reshape后列数为1，行数-1代表自动匹配计算，w_为 X by 1 的风险分配权重\n",
    "    cVar = np.dot( np.dot(w_.T, cov_),w_ )[0,0]   # 1byX * XbyX * Xby1\n",
    "    return cVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 2\n",
    "def getQuasiDiag( link ):\n",
    "    #执行Quasi-Diagonization的步骤，将因子暴露意义下相似的股票放到一起\n",
    "    #sort clustered items by distance\n",
    "    link = link.astype(int) #returns a copy of the array converted to the specified type\n",
    "    #sortIx记录每一层的组分\n",
    "    sortIx = pd.Series( [link[-1,0] , link[-1,1]] )\n",
    "    numItems = link[-1,3]    # number of original items,最后一行为root\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0 , sortIx.shape[0]*2 , 2)  #make space, step_length = 2\n",
    "        df0 = sortIx[ sortIx>=numItems ]                 #find clusters，把代表cluster而非originalelement的取出来\n",
    "        i = df0.index; j = df0.values - numItems\n",
    "        sortIx[i] = link[j,0]     #item 1\n",
    "        df0 = pd.Series( link[j,1] , index=i+1)  #循环变量在这里结束时+1\n",
    "        sortIx = sortIx.append(df0)  #item 2\n",
    "        sortIx = sortIx.sort_index()  #re-sort\n",
    "        sortIx.index = range( sortIx.shape[0] ) #re-index\n",
    "    return sortIx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 3\n",
    "def getRecBipart( cov, sortIx ):\n",
    "    # Bottom-up: Define the variance of a continuous subset as the variance of an inverse-variance allocation\n",
    "    # Top-down: Split allocations btw adjacent subsets in inverse proportion to their aggregated variances\n",
    "    w = pd.Series(1 , index=sortIx)\n",
    "    cItems = [sortIx]     #initialize all items in one cluster\n",
    "    while len(cItems)>0:\n",
    "        cItems = [ i[j:k] for i in cItems for j,k in ( (0,len(i)//2),(len(i)//2,len(i)) ) if len(i)>1 ]\n",
    "        # above: bi-section\n",
    "        for i in range(0, len(cItems), 2):    #parse in pairs\n",
    "            cItems0 = cItems[i]   #cluster 1\n",
    "            cItems1 = cItems[i+1] #cluster 2\n",
    "            cVar0 = getClusterVar( cov,cItems0)\n",
    "            cVar1 = getClusterVar( cov,cItems1)\n",
    "            alpha = 1 - cVar0/(cVar0+cVar1)\n",
    "            w[cItems0] *= alpha    #weight1\n",
    "            w[cItems1] *= 1-alpha  #weight2\n",
    "        \n",
    "    return w   #返回最终标的权重Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 0\n",
    "def correlDist(corr):\n",
    "    # Output a distance matrix based on correlation, where 0<= d[i,j] <=1\n",
    "    #合理的距离测度，针对于由相关性定义的原始距离矩阵\n",
    "    dist = ( (1-corr)/2. )**.5\n",
    "    return dist\n",
    "\n",
    "def CalEucliDist(vec1,vec2):  \n",
    "    #Output euclidean distance\n",
    "    #vec1,vec2分别为两个np.array\n",
    "    eucli = np.sqrt( np.sum( np.square(vec1 - vec2) ) )  \n",
    "    return eucli \n",
    "\n",
    "def correlDist_ofDist1(dist):\n",
    "    # A distance matrix based on original dist matrix\n",
    "    # Compute the Euclidean distance btw any two col-vectors of dist\n",
    "    # Therefore, new mat is a distance defined over the ENTIRE metric space,\\\n",
    "    # Rather than a PARTICULAR cross-correlation pair!\n",
    "    dist_ofDist = np.empty( shape(dist) )\n",
    "    \n",
    "    for i in range( 0,shape(dist)(1) ):\n",
    "        for j in range( i+1,shape(dist)(1) ):\n",
    "            dist_ofDist[i,j] = CalEucliDist( dist[:,i],dist[:,j] )\n",
    "            dist_ofDist[j,i] = dist_ofDist[i,j]\n",
    "            \n",
    "    dist_ofDist = ssd.squareform(dist_ofDist)\n",
    "    return dist_ofDist  # 转换为CondensedForm的\n",
    "\n",
    "#Otherwise, 可以调用scipy包中高维空间距离矩阵的算法\n",
    "def correlDist_ofDist2(dist):\n",
    "    dist_ofDist = ssd.pdist( dist, 'euclidean' )  #返回CondensedForm，即只有上三角部分存成数组形式\n",
    "    \n",
    "    return dist_ofDist  # CondensedForm的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCorrMatrix( path, corr, labels=None):\n",
    "    # Heatmapping of the corr matri\n",
    "    if labels is None: labels = []\n",
    "    mpl.pcolor(corr)\n",
    "    mpl.colorbar()\n",
    "    mpl.yticks( np.arange(.5 , corr.shape[0]+.5) , labels )\n",
    "    mpl.xticks( np.arange(.5 , corr.shape[0]+.5) , labels )\n",
    "    mpl.savefig(path)\n",
    "    mpl.clf(); mpl.close()   # reset pylab\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generateData( nObs, sLength,size0 ,size1 , mu0, sigma0, sigma1F):\\n    # Time series of correlated variables\\n    # Uncorrelated Data\\n    #sigma0为原始序列的波动率；sigma1F为在原始序列基础上，衍生序列的波动率额外增益\\n    x = np.random.normal( mu0 ,sigma0 , size=(nObs ,size0)) #each row is a variable\\n    \\n    # Creating correlation btw the variables\\n    #五条原始的独立序列，五条衍生出的相关序列\\n    cols = [ random.randint(0 , size0-1) for i in range(size1)]\\n    y = x[: , cols] + np.random.normal( 0 ,sigma0*sigma1F ,size=(nObs , len(cols)))\\n    #再把衍生序列叠加到原始序列边上\\n    x = np.append(x ,y ,axis =1)\\n    \\n    # add common random shock\\n    point = np.random.randint(sLength,nObs-1,size=2)\\n    x[point,cols[-1]] = np.array([-0.5 ,2])\\n\\n    return x,cols\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def generateData( nObs, sLength,size0 ,size1 , mu0, sigma0, sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    # Uncorrelated Data\n",
    "    #sigma0为原始序列的波动率；sigma1F为在原始序列基础上，衍生序列的波动率额外增益\n",
    "    x = np.random.normal( mu0 ,sigma0 , size=(nObs ,size0)) #each row is a variable\n",
    "    \n",
    "    # Creating correlation btw the variables\n",
    "    #五条原始的独立序列，五条衍生出的相关序列\n",
    "    cols = [ random.randint(0 , size0-1) for i in range(size1)]\n",
    "    y = x[: , cols] + np.random.normal( 0 ,sigma0*sigma1F ,size=(nObs , len(cols)))\n",
    "    #再把衍生序列叠加到原始序列边上\n",
    "    x = np.append(x ,y ,axis =1)\n",
    "    \n",
    "    # add common random shock\n",
    "    point = np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[point,cols[-1]] = np.array([-0.5 ,2])\n",
    "\n",
    "    return x,cols\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateData( nObs, sLength,size0 ,size1 , mu0, sigma0, sigma1F):\n",
    "    # Time series of correlated variables\n",
    "    # Uncorrelated Data\n",
    "    #sigma0为原始序列的波动率；sigma1F为在原始序列基础上，衍生序列的波动率额外增益\n",
    "    x = np.random.normal( mu0 ,sigma0 , size=(nObs ,size0)) #each row is a variable\n",
    "    \n",
    "    # Creating correlation btw the variables\n",
    "    #五条原始的独立序列，五条衍生出的相关序列\n",
    "    cols = [ random.randint(0 , size0-1) for i in range(size1)]\n",
    "    y = x[: , cols] + np.random.normal( 0 ,sigma0*sigma1F ,size=(nObs , len(cols)))\n",
    "    #再把衍生序列叠加到原始序列边上\n",
    "    x = np.append(x ,y ,axis =1)\n",
    "    \n",
    "    # add common random shock\n",
    "    point = np.random.randint(sLength,nObs-1,size=2)\n",
    "    x[np.ix_(point , [cols[0],size0])] = np.array([ [-0.5,-0.5],[2,2]])\n",
    "    # add specific random shock\n",
    "    point = np.random.randint(sLength,nObs-1, size=2)\n",
    "    x[point,cols[-1]] = np.array([-0.5 ,2])\n",
    "\n",
    "    return x,cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHRP(cov, corr):\n",
    "   \n",
    "    #Construct a hierarchical portfolio\n",
    "    corr,cov = pd.DataFrame(corr),pd.DataFrame(cov)\n",
    "    dist = correlDist(corr)\n",
    "    \n",
    "    new_dist = correlDist_ofDist2(dist)\n",
    "    \n",
    "    link = sch.linkage(new_dist,'single')\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "    \n",
    "    return hrp.sort_index()\n",
    "\n",
    "\n",
    "def getIVP( cov , **kargs):\n",
    "   \n",
    "    # compute the inverse-variance portfolio\n",
    "    ivp = 1 / np.diag(cov)   #ivp为一维数组\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp               #最终方差大的部分ivp较小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "描述了不同市场波动率程度和shock来源的特异性或普遍性\n",
    "多次实验参数设置为：sigma0取0.02，0.08，0.20；  越大波动环境越强\n",
    "              sigma1取0.05，0.20，0.50；  越大特异性shock越强\n",
    "    mu0 = 0,因其表示收益序列的均值\n",
    "    模拟次数iters = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulation(numIters = 600 , nObs = 520, size0 = 5,size1 = 5, mu0 = 0, sigma0 = 0.02,\\\n",
    "         sigma1F = 0.05, sLength =260, rebal = 22): #总长520\n",
    "    #monte carlo experiment \n",
    "    #默认参数为：最低波动率环境，特异性冲击最少\n",
    "    stats = {'HRP': pd.Series(),'IVP': pd.Series(),'EQUAL':pd.Series(),\\\n",
    "             'HRP_VaR': pd.Series(),'IVP_VaR': pd.Series(),'EQUAL_VaR':pd.Series(),\\\n",
    "             'HRP_TailRatio':pd.Series(),'IVP_TailRatio':pd.Series(),'EQUAL_TailRatio':pd.Series()}\n",
    "         #存储不同配置方法的实验统计结果\n",
    "         #字典（key为方法，value为series）后来转为dataframe\n",
    "    \n",
    "    pointers = range(sLength ,nObs ,rebal) #从sLength起始，到nObs截止，步长为22天，月度\n",
    "    for numIter in range(int(numIters)):\n",
    "        print(numIter)\n",
    "        \n",
    "        #一、准备数据 for one single experiment\n",
    "        x, cols = generateData(nObs ,sLength ,size0,size1, mu0 ,sigma0 , sigma1F)\n",
    "        \n",
    "        r = pd.DataFrame( columns = ['HRP','IVP','EQUAL'], index=range(sLength,nObs))\n",
    "\n",
    "        \n",
    "        #二、样本内资产组合\n",
    "        for pointer in pointers:\n",
    "            x_ = x[pointer - sLength : pointer]  #每段窗长为sLength，为pointer那天向前的一段时间\n",
    "            cov_  = np.cov(x_ ,rowvar = 0)       #为时间序列协方差矩阵\n",
    "            corr_ = np.corrcoef(x_, rowvar = 0)   #为时间序列相关性矩阵\n",
    "          \n",
    "            #三、样本外资产组合\n",
    "            x_ = x[pointer : pointer+rebal]      #替换为样本外一个月\n",
    "            w1 = getHRP(cov=cov_, corr=corr_)  #输入的为样本内参数，得到分配权重\n",
    "            w2 = getIVP(cov=cov_, corr=corr_)\n",
    "            size2 = size0+size1\n",
    "            w2 = pd.Series(w2)\n",
    "            w3 = pd.Series([1/size2,1/size2,1/size2,1/size2,1/size2,1/size2,1/size2,1/size2,1/size2,1/size2])\n",
    "        \n",
    "            r.loc[pointer:pointer + rebal - 1,'HRP'] = np.dot(x_, w1)\n",
    "            r.loc[pointer:pointer + rebal - 1,'IVP'] = np.dot(x_, w2)\n",
    "            r.loc[pointer:pointer + rebal - 1,'EQUAL'] = np.dot(x_, w3)\n",
    "            \n",
    "        # 四、 Evaluate and store results\n",
    "            \n",
    "        #不同方案 最终受益 和 99%VaR均值 和 TailRatio均值\n",
    "        #先取出对应key的value，即一个series，在对series的元素赋值\n",
    "        r_ = r['HRP'].reset_index(drop=True)\n",
    "        p_ = (1 + r_).cumprod()\n",
    "        stats['HRP'].loc[numIter] = p_.iloc[-1] - 1         # terminal return\n",
    "        stats['HRP_VaR'].loc[numIter] = computeVaR(r_)      # VaR\n",
    "        stats['HRP_TailRatio'].loc[numIter] = tail_ratio(r_)# tail_ratio derived from return series\n",
    "            \n",
    "        r_ = r['IVP'].reset_index(drop=True)\n",
    "        p_ = (1 + r_).cumprod()\n",
    "        stats['IVP'].loc[numIter] = p_.iloc[-1] - 1  \n",
    "        stats['IVP_VaR'].loc[numIter] = computeVaR(r_)   \n",
    "        stats['IVP_TailRatio'].loc[numIter] = tail_ratio(r_)\n",
    "        \n",
    "        r_ = r['EQUAL'].reset_index(drop=True)\n",
    "        p_ = (1 + r_).cumprod()\n",
    "        stats['EQUAL'].loc[numIter] = p_.iloc[-1] - 1  \n",
    "        stats['EQUAL_VaR'].loc[numIter] = computeVaR(r_)     \n",
    "        stats['EQUAL_TailRatio'].loc[numIter] = tail_ratio(r_)    \n",
    "            \n",
    "    #五、报告结果\n",
    "    stats = pd.DataFrame.from_dict(stats , orient = 'columns')\n",
    "    stats.to_csv('stats_1017_%s_%s.csv' % ( str(sigma0), str(sigma1F)))\n",
    "    \n",
    "    print(\"=================== Out-of-sample std & var =====================\")\n",
    "    std_of_MC ,var_of_MC = stats[['HRP','IVP','EQUAL']].std(), stats[['HRP','IVP','EQUAL']].var()\n",
    "    print(std_of_MC,var_of_MC)\n",
    "    print(\"====================== VaR Mean ======================\")\n",
    "    mean_of_VaR = stats[['HRP_VaR','IVP_VaR','EQUAL_VaR']].mean()\n",
    "    print(mean_of_VaR)\n",
    "    print(\"===================== TailRatio Mean =====================\")\n",
    "    mean_of_TR = stats[['HRP_TailRatio','IVP_TailRatio','EQUAL_TailRatio']].mean()\n",
    "    print(mean_of_TR)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
